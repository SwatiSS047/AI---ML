{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Popular Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Predict employee attrition based on job satisfaction and salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of the generated dataset:\n",
      "   job_satisfaction  monthly_salary  years_at_company  performance_rating  \\\n",
      "0                 4            5164                 6                   2   \n",
      "1                 5            6088                 7                   3   \n",
      "2                 3            5721                 4                   3   \n",
      "3                 5            5335                 7                   3   \n",
      "4                 5            3814                 9                   2   \n",
      "\n",
      "   overtime  promotion_last_5years  attrition  \n",
      "0         0                      0          0  \n",
      "1         0                      1          1  \n",
      "2         1                      1          0  \n",
      "3         0                      0          0  \n",
      "4         0                      0          0  \n",
      "\n",
      "Attrition distribution:\n",
      "attrition\n",
      "0    926\n",
      "1     74\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Training set size: 700 samples\n",
      "Testing set size: 300 samples\n",
      "--------------------------------------------------\n",
      "Starting Hyperparameter Tuning for Random Forest...\n",
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Generate Sample Data\n",
    "# In a real scenario, you would load your HR dataset.\n",
    "# Example: df = pd.read_csv('hr_data.csv')\n",
    "\n",
    "np.random.seed(42) # for reproducibility\n",
    "\n",
    "num_employees = 1000\n",
    "\n",
    "# Features\n",
    "job_satisfaction = np.random.randint(1, 6, size=num_employees) # 1 (low) to 5 (high)\n",
    "monthly_salary = np.random.normal(loc=5000, scale=1500, size=num_employees)\n",
    "monthly_salary = np.clip(monthly_salary, 2500, 10000).astype(int) # Realistic salary range\n",
    "years_at_company = np.random.randint(1, 10, size=num_employees) # 1 to 9 years\n",
    "performance_rating = np.random.randint(1, 4, size=num_employees) # 1 (low) to 3 (high)\n",
    "overtime = np.random.choice([0, 1], size=num_employees, p=[0.7, 0.3]) # 0: No, 1: Yes\n",
    "promotion_last_5years = np.random.choice([0, 1], size=num_employees, p=[0.8, 0.2]) # 0: No, 1: Yes\n",
    "\n",
    "\n",
    "# Simulate attrition (target variable)\n",
    "# Logic: Higher attrition risk with low job satisfaction, lower salary, high overtime, no recent promotion, high years at company (if stuck)\n",
    "attrition_probability = (\n",
    "    0.15 # Baseline attrition probability\n",
    "    - (job_satisfaction * 0.03) # Higher satisfaction, lower attrition\n",
    "    - (monthly_salary / 10000) * 0.05 # Higher salary, lower attrition\n",
    "    + (years_at_company * 0.01) # Longer tenure might increase attrition if no growth\n",
    "    - (performance_rating * 0.02) # Higher performance, lower attrition (often due to promotion)\n",
    "    + (overtime * 0.10) # Overtime increases attrition\n",
    "    - (promotion_last_5years * 0.10) # Promotion decreases attrition\n",
    "    + np.random.normal(0, 0.03, size=num_employees) # Add some noise\n",
    ")\n",
    "\n",
    "# Ensure probabilities are within [0, 1] and slightly skewed towards no attrition\n",
    "attrition_probability = np.clip(attrition_probability, 0.02, 0.50) # Keep attrition rate relatively low\n",
    "\n",
    "# Generate 'attrition' (1 for attrition, 0 for no attrition)\n",
    "attrition = (np.random.rand(num_employees) < attrition_probability).astype(int)\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'job_satisfaction': job_satisfaction,\n",
    "    'monthly_salary': monthly_salary,\n",
    "    'years_at_company': years_at_company,\n",
    "    'performance_rating': performance_rating,\n",
    "    'overtime': overtime,\n",
    "    'promotion_last_5years': promotion_last_5years,\n",
    "    'attrition': attrition\n",
    "})\n",
    "\n",
    "print(\"Sample of the generated dataset:\")\n",
    "print(data.head())\n",
    "print(\"\\nAttrition distribution:\")\n",
    "print(data['attrition'].value_counts())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 2. Define Features (X) and Target (y)\n",
    "X = data.drop('attrition', axis=1)\n",
    "y = data['attrition']\n",
    "\n",
    "# 3. Split Data into Training and Testing Sets\n",
    "# Using stratify=y is crucial for imbalanced datasets like attrition\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)} samples\")\n",
    "print(f\"Testing set size: {len(X_test)} samples\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- Hyperparameter Tuning with Cross-Validation (GridSearchCV) ---\n",
    "print(\"Starting Hyperparameter Tuning for Random Forest...\")\n",
    "\n",
    "# Define the parameter grid to search\n",
    "# These are common parameters to tune for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200], # Number of trees in the forest\n",
    "    'max_depth': [5, 10, 15, None], # Maximum depth of the tree\n",
    "    'min_samples_leaf': [1, 5, 10], # Minimum number of samples required to be at a leaf node\n",
    "    'min_samples_split': [2, 5, 10], # Minimum number of samples required to split an internal node\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Use StratifiedKFold for cross-validation, essential for imbalanced classification\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=cv_strategy,\n",
    "    scoring='roc_auc', # ROC AUC is a robust metric for imbalanced classes\n",
    "    n_jobs=-1, # Use all available CPU cores\n",
    "    verbose=1 # Show progress\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nHyperparameter Tuning Complete!\")\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best ROC AUC score (cross-validated): {grid_search.best_score_:.4f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 4. Train the Best Random Forest Model\n",
    "model = grid_search.best_estimator_ # Get the best model found by GridSearchCV\n",
    "print(f\"Using best model with parameters: {model.get_params()}\")\n",
    "# The model is already fitted from GridSearchCV, but explicit fit is fine.\n",
    "# model.fit(X_train, y_train) # Not strictly necessary if using best_estimator_ after fit\n",
    "\n",
    "print(\"Best Random Forest Model Trained Successfully!\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 5. Make Predictions on the Test Set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1] # Probability of attrition (class 1)\n",
    "\n",
    "print(\"Sample Predictions on Test Set:\")\n",
    "results = pd.DataFrame({\n",
    "    'Actual Attrition': y_test,\n",
    "    'Predicted Attrition': y_pred,\n",
    "    'Attrition Probability': y_pred_proba\n",
    "})\n",
    "print(results.head(10)) # Display first 10 predictions\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 6. Evaluate Model Performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred, target_names=['No Attrition', 'Attrition'])\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Model Accuracy on Test Set: {accuracy:.4f}\")\n",
    "print(f\"Model ROC AUC Score on Test Set: {roc_auc:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Visualize Confusion Matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Predicted No Attrition', 'Predicted Attrition'],\n",
    "            yticklabels=['Actual No Attrition', 'Actual Attrition'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Employee Attrition Prediction')\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 7. Feature Importance\n",
    "print(\"Feature Importances (how much each feature contributes to prediction):\")\n",
    "feature_importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "print(feature_importances)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=feature_importances.values, y=feature_importances.index, palette='viridis')\n",
    "plt.title('Feature Importances for Attrition Prediction')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 8. Predict for a New Employee with Input Validation\n",
    "def predict_new_employee_attrition(job_satisfaction, monthly_salary, years_at_company,\n",
    "                                   performance_rating, overtime, promotion_last_5years,\n",
    "                                   model, feature_columns):\n",
    "    # Input Validation (basic checks for logical ranges)\n",
    "    if not (1 <= job_satisfaction <= 5):\n",
    "        print(f\"Warning: Job Satisfaction {job_satisfaction} is outside the typical range (1-5).\")\n",
    "    if not (monthly_salary >= 0):\n",
    "        print(f\"Warning: Monthly Salary {monthly_salary} should be non-negative.\")\n",
    "    if not (years_at_company >= 0):\n",
    "        print(f\"Warning: Years at Company {years_at_company} should be non-negative.\")\n",
    "    if not (1 <= performance_rating <= 3):\n",
    "        print(f\"Warning: Performance Rating {performance_rating} is outside the typical range (1-3).\")\n",
    "    if overtime not in [0, 1]:\n",
    "        print(f\"Warning: Overtime {overtime} should be 0 (No) or 1 (Yes).\")\n",
    "    if promotion_last_5years not in [0, 1]:\n",
    "        print(f\"Warning: Promotion Last 5 Years {promotion_last_5years} should be 0 (No) or 1 (Yes).\")\n",
    "\n",
    "    new_employee_data = pd.DataFrame([[\n",
    "        job_satisfaction, monthly_salary, years_at_company,\n",
    "        performance_rating, overtime, promotion_last_5years\n",
    "    ]], columns=feature_columns)\n",
    "\n",
    "    prediction = model.predict(new_employee_data)[0]\n",
    "    probability = model.predict_proba(new_employee_data)[0][1] # Probability of attrition (class 1)\n",
    "\n",
    "    status = \"HIGH Risk of Attrition\" if prediction == 1 else \"LOW Risk of Attrition\"\n",
    "    return status, probability\n",
    "\n",
    "# Test with some new employee examples\n",
    "print(\"\\nPredicting for new employees with input validation:\")\n",
    "\n",
    "# Example 1: High risk employee (low satisfaction, high overtime, no promotion)\n",
    "status1, prob1 = predict_new_employee_attrition(\n",
    "    job_satisfaction=2, monthly_salary=4000, years_at_company=3,\n",
    "    performance_rating=2, overtime=1, promotion_last_5years=0,\n",
    "    model=model, feature_columns=X.columns\n",
    ")\n",
    "print(f\"Employee 1 - Predicted Status: {status1}, Probability of Attrition: {prob1:.4f}\")\n",
    "\n",
    "# Example 2: Low risk employee (high satisfaction, good salary, promotion)\n",
    "status2, prob2 = predict_new_employee_attrition(\n",
    "    job_satisfaction=5, monthly_salary=8000, years_at_company=5,\n",
    "    performance_rating=3, overtime=0, promotion_last_5years=1,\n",
    "    model=model, feature_columns=X.columns\n",
    ")\n",
    "print(f\"Employee 2 - Predicted Status: {status2}, Probability of Attrition: {prob2:.4f}\")\n",
    "\n",
    "# Example 3: Borderline case (moderate satisfaction, average salary, no overtime)\n",
    "status3, prob3 = predict_new_employee_attrition(\n",
    "    job_satisfaction=3, monthly_salary=5500, years_at_company=4,\n",
    "    performance_rating=2, overtime=0, promotion_last_5years=0,\n",
    "    model=model, feature_columns=X.columns\n",
    ")\n",
    "print(f\"Employee 3 - Predicted Status: {status3}, Probability of Attrition: {prob3:.4f}\")\n",
    "\n",
    "# Example 4: With a problematic input (will trigger warning)\n",
    "status4, prob4 = predict_new_employee_attrition(\n",
    "    job_satisfaction=6, monthly_salary=-1000, years_at_company=12,\n",
    "    performance_rating=0, overtime=2, promotion_last_5years=-1,\n",
    "    model=model, feature_columns=X.columns\n",
    ")\n",
    "print(f\"Employee 4 - Predicted Status: {status4}, Probability of Attrition: {prob4:.4f}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n--- Summary of Random Forest Features and Best Practices ---\")\n",
    "print(\"1. **Ensemble Learning**: Random Forest is an ensemble method, building multiple decision trees and combining their predictions. This reduces overfitting and improves generalization compared to a single decision tree.\")\n",
    "print(\"2. **Robustness to Overfitting**: By averaging multiple trees, Random Forest is less prone to overfitting than individual decision trees.\")\n",
    "print(\"3. **Feature Importance**: It provides a reliable measure of feature importance, helping identify which factors are most influential in predicting attrition.\")\n",
    "print(\"4. **Hyperparameter Tuning**: `GridSearchCV` with `StratifiedKFold` is used for systematic hyperparameter tuning, finding optimal `n_estimators`, `max_depth`, `min_samples_leaf`, etc., to get the best model performance.\")\n",
    "print(\"5. **Evaluation Metrics**: ROC AUC is used as the primary scoring metric during tuning, as it's more robust for imbalanced datasets typical in attrition prediction.\")\n",
    "print(\"6. **Input Validation**: Added basic input validation to the prediction function to ensure new data conforms to expected ranges, making the model more robust in a practical setting.\")\n",
    "print(\"7. **Scalability**: Random Forests are computationally efficient and can handle large datasets well.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Classify types of wine based on chemical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Wine Dataset Info:\n",
      "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
      "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
      "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
      "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
      "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
      "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
      "\n",
      "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
      "0        3.06                  0.28             2.29             5.64  1.04   \n",
      "1        2.76                  0.26             1.28             4.38  1.05   \n",
      "2        3.24                  0.30             2.81             5.68  1.03   \n",
      "3        3.49                  0.24             2.18             7.80  0.86   \n",
      "4        2.69                  0.39             1.82             4.32  1.04   \n",
      "\n",
      "   od280/od315_of_diluted_wines  proline  \n",
      "0                          3.92   1065.0  \n",
      "1                          3.40   1050.0  \n",
      "2                          3.17   1185.0  \n",
      "3                          3.45   1480.0  \n",
      "4                          2.93    735.0  \n",
      "\n",
      "Features: ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "Target classes: ['Class 0', 'Class 1', 'Class 2']\n",
      "Target distribution: \n",
      "0    59\n",
      "1    71\n",
      "2    48\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Training set size: 124 samples\n",
      "Testing set size: 54 samples\n",
      "--------------------------------------------------\n",
      "Starting Hyperparameter Tuning for Random Forest...\n",
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 62\u001b[0m\n\u001b[1;32m     52\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m     53\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mRandomForestClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[1;32m     54\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# Show progress\u001b[39;00m\n\u001b[1;32m     59\u001b[0m )\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Fit GridSearchCV to the training data\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHyperparameter Tuning Complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1571\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    965\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    966\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    967\u001b[0m         )\n\u001b[1;32m    968\u001b[0m     )\n\u001b[0;32m--> 970\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    993\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Generate Sample Data\n",
    "# In a real scenario, you would load a dataset containing loan application details.\n",
    "# Example: df = pd.read_csv('loan_applications.csv')\n",
    "\n",
    "np.random.seed(42) # for reproducibility\n",
    "\n",
    "num_applicants = 1200 # You can temporarily reduce this for faster testing, e.g., 500\n",
    "\n",
    "# Features\n",
    "credit_score = np.random.normal(loc=680, scale=70, size=num_applicants).astype(int)\n",
    "credit_score = np.clip(credit_score, 300, 850) # Ensure scores are within valid FICO range\n",
    "\n",
    "annual_income_k = np.random.normal(loc=80, scale=40, size=num_applicants) # in thousands\n",
    "annual_income_k = np.clip(annual_income_k, 30, 250) # Min 30k, Max 250k\n",
    "\n",
    "loan_amount_k = np.random.normal(loc=250, scale=100, size=num_applicants) # in thousands\n",
    "loan_amount_k = np.clip(loan_amount_k, 50, 700) # Min 50k, Max 700k\n",
    "\n",
    "employment_status = np.random.choice(['Employed', 'Self-Employed', 'Unemployed', 'Retired'],\n",
    "                                     size=num_applicants, p=[0.7, 0.15, 0.1, 0.05])\n",
    "\n",
    "property_value_k = np.random.normal(loc=300, scale=150, size=num_applicants) # in thousands\n",
    "property_value_k = np.clip(property_value_k, 100, 1000) # Min 100k, Max 1000k\n",
    "\n",
    "debt_to_income_ratio = np.random.normal(loc=0.36, scale=0.1)\n",
    "debt_to_income_ratio = np.clip(debt_to_income_ratio, 0.05, 0.6) # Realistic DTI range\n",
    "\n",
    "# Simulate loan approval (target variable)\n",
    "# Logic: Approval more likely with high credit score, high income, low loan amount relative to property value,\n",
    "# stable employment, and low DTI.\n",
    "approval_probability = (\n",
    "    0.6 # Baseline approval probability\n",
    "    + (credit_score / 1000) * 0.2 # Higher credit score, higher chance\n",
    "    + (annual_income_k / 200) * 0.15 # Higher income, higher chance\n",
    "    - (loan_amount_k / property_value_k) * 0.2 # Higher loan/property ratio, lower chance\n",
    "    - (debt_to_income_ratio * 0.4) # Higher DTI, lower chance\n",
    "    + np.where(employment_status == 'Employed', 0.1, 0) # Employed has higher chance\n",
    "    + np.where(employment_status == 'Self-Employed', 0.05, 0) # Self-employed slightly higher\n",
    "    - np.where(employment_status == 'Unemployed', 0.3, 0) # Unemployed much lower\n",
    "    + np.random.normal(0, 0.05, size=num_applicants) # Add some noise\n",
    ")\n",
    "\n",
    "# Ensure probabilities are within [0, 1]\n",
    "approval_probability = np.clip(approval_probability, 0.05, 0.95)\n",
    "\n",
    "# Generate 'loan_approved' (1 for approved, 0 for denied)\n",
    "loan_approved = (np.random.rand(num_applicants) < approval_probability).astype(int)\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'credit_score': credit_score,\n",
    "    'annual_income_k': annual_income_k,\n",
    "    'loan_amount_k': loan_amount_k,\n",
    "    'employment_status': employment_status,\n",
    "    'property_value_k': property_value_k,\n",
    "    'debt_to_income_ratio': debt_to_income_ratio,\n",
    "    'loan_approved': loan_approved\n",
    "})\n",
    "\n",
    "print(\"Sample of the generated dataset:\")\n",
    "print(data.head())\n",
    "print(\"\\nLoan Approval distribution:\")\n",
    "print(data['loan_approved'].value_counts())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 2. Preprocess Data: One-hot encode categorical features\n",
    "data_encoded = pd.get_dummies(data, columns=['employment_status'], drop_first=False) # Keep all dummies\n",
    "\n",
    "# Define Features (X) and Target (y)\n",
    "X = data_encoded.drop('loan_approved', axis=1)\n",
    "y = data_encoded['loan_approved'] # Target variable\n",
    "\n",
    "# Get feature names after one-hot encoding for consistency\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "print(\"\\nSample of the encoded dataset (features only):\")\n",
    "print(X.head())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 3. Split Data into Training and Testing Sets\n",
    "# Using stratify=y is crucial for imbalanced datasets like loan approval/denial\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)} samples\")\n",
    "print(f\"Testing set size: {len(X_test)} samples\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- Hyperparameter Tuning with Cross-Validation (GridSearchCV) ---\n",
    "print(\"Starting Hyperparameter Tuning for Random Forest...\")\n",
    "\n",
    "# Define the parameter grid to search\n",
    "# SUGGESTION: If GridSearchCV takes too long, reduce the number of options in param_grid.\n",
    "# For example, use:\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200],\n",
    "#     'max_depth': [5, 10],\n",
    "#     'min_samples_leaf': [1, 5],\n",
    "#     'criterion': ['gini']\n",
    "# }\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300], # Number of trees in the forest\n",
    "    'max_depth': [5, 10, 15, None], # Maximum depth of the tree\n",
    "    'min_samples_leaf': [1, 5, 10], # Minimum number of samples required to be at a leaf node\n",
    "    'min_samples_split': [2, 5, 10], # Minimum number of samples required to split an internal node\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Use StratifiedKFold for cross-validation, essential for imbalanced classification\n",
    "# SUGGESTION: If it's still too slow, reduce n_splits, e.g., n_splits=3\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=cv_strategy,\n",
    "    scoring='roc_auc', # ROC AUC is a robust metric for imbalanced classes\n",
    "    n_jobs=-1, # Use all available CPU cores. This can be slow if you have many cores and limited RAM.\n",
    "              # If you suspect memory issues, try n_jobs=1 or n_jobs=2.\n",
    "    verbose=1 # Show progress\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "# This is the line that might be interrupted if it takes too long.\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nHyperparameter Tuning Complete!\")\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best ROC AUC score (cross-validated): {grid_search.best_score_:.4f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 4. Train the Best Random Forest Model\n",
    "model = grid_search.best_estimator_ # Get the best model found by GridSearchCV\n",
    "print(f\"Using best model with parameters: {model.get_params()}\")\n",
    "# The model is already fitted from GridSearchCV's .fit() call\n",
    "\n",
    "print(\"Best Random Forest Model Trained Successfully!\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 5. Make Predictions on the Test Set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1] # Probability of loan approval (class 1)\n",
    "\n",
    "print(\"Sample Predictions on Test Set:\")\n",
    "results = pd.DataFrame({\n",
    "    'Actual Approval': y_test,\n",
    "    'Predicted Approval': y_pred,\n",
    "    'Approval Probability': y_pred_proba\n",
    "})\n",
    "print(results.head(10)) # Display first 10 predictions\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 6. Evaluate Model Performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred, target_names=['Denied', 'Approved'])\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Model Accuracy on Test Set: {accuracy:.4f}\")\n",
    "print(f\"Model ROC AUC Score on Test Set: {roc_auc:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Visualize Confusion Matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Predicted Denied', 'Predicted Approved'],\n",
    "            yticklabels=['Actual Denied', 'Actual Approved'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Housing Loan Approval Prediction')\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 7. Feature Importance\n",
    "print(\"Feature Importances (how much each feature contributes to prediction):\")\n",
    "feature_importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "print(feature_importances)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.barplot(x=feature_importances.values, y=feature_importances.index, palette='crest')\n",
    "plt.title('Feature Importances for Housing Loan Approval Prediction')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 8. Predict for a New Applicant with Input Validation\n",
    "def predict_loan_approval(credit_score, annual_income_k, loan_amount_k, employment_status,\n",
    "                          property_value_k, debt_to_income_ratio,\n",
    "                          model, feature_columns):\n",
    "    # Input Validation (basic checks for logical ranges)\n",
    "    if not (300 <= credit_score <= 850):\n",
    "        print(f\"Warning: Credit Score {credit_score} is outside the typical range (300-850).\")\n",
    "    if not (annual_income_k >= 0):\n",
    "        print(f\"Warning: Annual Income {annual_income_k} should be non-negative.\")\n",
    "    if not (loan_amount_k >= 0):\n",
    "        print(f\"Warning: Loan Amount {loan_amount_k} should be non-negative.\")\n",
    "    if employment_status not in ['Employed', 'Self-Employed', 'Unemployed', 'Retired']:\n",
    "        print(f\"Warning: Invalid Employment Status '{employment_status}'. Must be one of: Employed, Self-Employed, Unemployed, Retired.\")\n",
    "        return \"Invalid Input\", 0.0\n",
    "    if not (property_value_k >= 0):\n",
    "        print(f\"Warning: Property Value {property_value_k} should be non-negative.\")\n",
    "    if not (0 <= debt_to_income_ratio <= 1):\n",
    "        print(f\"Warning: Debt-to-Income Ratio {debt_to_income_ratio} is outside the typical range (0-1).\")\n",
    "\n",
    "    # Create a DataFrame for the new applicant\n",
    "    new_applicant_df = pd.DataFrame([{\n",
    "        'credit_score': credit_score,\n",
    "        'annual_income_k': annual_income_k,\n",
    "        'loan_amount_k': loan_amount_k,\n",
    "        'employment_status': employment_status,\n",
    "        'property_value_k': property_value_k,\n",
    "        'debt_to_income_ratio': debt_to_income_ratio\n",
    "    }])\n",
    "\n",
    "    # One-hot encode the new applicant's categorical features\n",
    "    new_applicant_encoded = pd.get_dummies(new_applicant_df, columns=['employment_status'], drop_first=False)\n",
    "\n",
    "    # Reindex to ensure columns match the training data's feature_columns order\n",
    "    # Fill missing columns (e.g., other employment_status categories not present in this single sample) with 0\n",
    "    new_applicant_processed = new_applicant_encoded.reindex(columns=feature_columns, fill_value=0)\n",
    "\n",
    "    # Predict\n",
    "    prediction = model.predict(new_applicant_processed)[0]\n",
    "    probability = model.predict_proba(new_applicant_processed)[0][1] # Probability of approval (class 1)\n",
    "\n",
    "    status = \"APPROVED\" if prediction == 1 else \"DENIED\"\n",
    "    return status, probability\n",
    "\n",
    "# Test with some new applicant examples\n",
    "print(\"\\nPredicting for new loan applicants with input validation:\")\n",
    "\n",
    "# Example 1: High chance of approval\n",
    "status1, prob1 = predict_loan_approval(\n",
    "    credit_score=750, annual_income_k=120, loan_amount_k=200, employment_status='Employed',\n",
    "    property_value_k=400, debt_to_income_ratio=0.25,\n",
    "    model=model, feature_columns=X.columns\n",
    ")\n",
    "print(f\"Applicant 1 - Predicted Status: {status1}, Probability of Approval: {prob1:.4f}\")\n",
    "\n",
    "# Example 2: High chance of denial\n",
    "status2, prob2 = predict_loan_approval(\n",
    "    credit_score=550, annual_income_k=40, loan_amount_k=300, employment_status='Unemployed',\n",
    "    property_value_k=350, debt_to_income_ratio=0.55,\n",
    "    model=model, feature_columns=X.columns\n",
    ")\n",
    "print(f\"Applicant 2 - Predicted Status: {status2}, Probability of Approval: {prob2:.4f}\")\n",
    "\n",
    "# Example 3: Borderline case\n",
    "status3, prob3 = predict_loan_approval(\n",
    "    credit_score=680, annual_income_k=70, loan_amount_k=280, employment_status='Self-Employed',\n",
    "    property_value_k=300, debt_to_income_ratio=0.40,\n",
    "    model=model, feature_columns=X.columns\n",
    ")\n",
    "print(f\"Applicant 3 - Predicted Status: {status3}, Probability of Approval: {prob3:.4f}\")\n",
    "\n",
    "# Example 4: With problematic input (will trigger warnings)\n",
    "status4, prob4 = predict_loan_approval(\n",
    "    credit_score=900, annual_income_k=-50, loan_amount_k=1000, employment_status='Student', # Invalid status\n",
    "    property_value_k=0, debt_to_income_ratio=1.5,\n",
    "    model=model, feature_columns=X.columns\n",
    ")\n",
    "print(f\"Applicant 4 - Predicted Status: {status4}, Probability of Approval: {prob4:.4f}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n--- Summary of Housing Loan Approval Prediction with Random Forest ---\")\n",
    "print(\"1. **Data Generation**: Simulated realistic financial and personal data for loan applicants, including categorical employment status.\")\n",
    "print(\"2. **Preprocessing**: Used one-hot encoding for the `employment_status` categorical feature, which is essential for Random Forest.\")\n",
    "print(\"3. **Hyperparameter Tuning**: Applied `GridSearchCV` with `StratifiedKFold` to find the optimal Random Forest parameters, focusing on `roc_auc` for robust evaluation in binary classification.\")\n",
    "print(\"   - *Note on KeyboardInterrupt*: If this step takes too long, reduce the complexity of the `param_grid` or `n_splits` as suggested in the comments.\")\n",
    "print(\"4. **Model Evaluation**: Provided comprehensive evaluation metrics including accuracy, confusion matrix, classification report, and ROC curve, which are critical for assessing model performance in a sensitive application like loan approval.\")\n",
    "print(\"5. **Feature Importance**: Identified the most influential factors (e.g., credit score, DTI, income) in determining loan approval, offering valuable insights for lenders.\")\n",
    "print(\"6. **Prediction Function with Validation**: Developed a `predict_loan_approval` function that includes input validation and correctly handles one-hot encoding for new, unseen data, making it more robust for deployment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Predict housing loan approval based on financial and personal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Generate Sample Data\n",
    "# In a real scenario, you would load a dataset containing loan application details.\n",
    "# Example: df = pd.read_csv('loan_applications.csv')\n",
    "\n",
    "np.random.seed(42) # for reproducibility\n",
    "\n",
    "num_applicants = 1200\n",
    "\n",
    "# Features\n",
    "credit_score = np.random.normal(loc=680, scale=70, size=num_applicants).astype(int)\n",
    "credit_score = np.clip(credit_score, 300, 850) # Ensure scores are within valid FICO range\n",
    "\n",
    "annual_income_k = np.random.normal(loc=80, scale=40, size=num_applicants) # in thousands\n",
    "annual_income_k = np.clip(annual_income_k, 30, 250) # Min 30k, Max 250k\n",
    "\n",
    "loan_amount_k = np.random.normal(loc=250, scale=100, size=num_applicants) # in thousands\n",
    "loan_amount_k = np.clip(loan_amount_k, 50, 700) # Min 50k, Max 700k\n",
    "\n",
    "employment_status = np.random.choice(['Employed', 'Self-Employed', 'Unemployed', 'Retired'],\n",
    "                                     size=num_applicants, p=[0.7, 0.15, 0.1, 0.05])\n",
    "\n",
    "property_value_k = np.random.normal(loc=300, scale=150, size=num_applicants) # in thousands\n",
    "property_value_k = np.clip(property_value_k, 100, 1000) # Min 100k, Max 1000k\n",
    "\n",
    "debt_to_income_ratio = np.random.normal(loc=0.36, scale=0.1)\n",
    "debt_to_income_ratio = np.clip(debt_to_income_ratio, 0.05, 0.6) # Realistic DTI range\n",
    "\n",
    "# Simulate loan approval (target variable)\n",
    "# Logic: Approval more likely with high credit score, high income, low loan amount relative to property value,\n",
    "# stable employment, and low DTI.\n",
    "approval_probability = (\n",
    "    0.6 # Baseline approval probability\n",
    "    + (credit_score / 1000) * 0.2 # Higher credit score, higher chance\n",
    "    + (annual_income_k / 200) * 0.15 # Higher income, higher chance\n",
    "    - (loan_amount_k / property_value_k) * 0.2 # Higher loan/property ratio, lower chance\n",
    "    - (debt_to_income_ratio * 0.4) # Higher DTI, lower chance\n",
    "    + np.where(employment_status == 'Employed', 0.1, 0) # Employed has higher chance\n",
    "    + np.where(employment_status == 'Self-Employed', 0.05, 0) # Self-employed slightly higher\n",
    "    - np.where(employment_status == 'Unemployed', 0.3, 0) # Unemployed much lower\n",
    "    + np.random.normal(0, 0.05, size=num_applicants) # Add some noise\n",
    ")\n",
    "\n",
    "# Ensure probabilities are within [0, 1]\n",
    "approval_probability = np.clip(approval_probability, 0.05, 0.95)\n",
    "\n",
    "# Generate 'loan_approved' (1 for approved, 0 for denied)\n",
    "loan_approved = (np.random.rand(num_applicants) < approval_probability).astype(int)\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'credit_score': credit_score,\n",
    "    'annual_income_k': annual_income_k,\n",
    "    'loan_amount_k': loan_amount_k,\n",
    "    'employment_status': employment_status,\n",
    "    'property_value_k': property_value_k,\n",
    "    'debt_to_income_ratio': debt_to_income_ratio,\n",
    "    'loan_approved': loan_approved\n",
    "})\n",
    "\n",
    "print(\"Sample of the generated dataset:\")\n",
    "print(data.head())\n",
    "print(\"\\nLoan Approval distribution:\")\n",
    "print(data['loan_approved'].value_counts())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 2. Preprocess Data: One-hot encode categorical features\n",
    "data_encoded = pd.get_dummies(data, columns=['employment_status'], drop_first=False) # Keep all dummies\n",
    "\n",
    "# Define Features (X) and Target (y)\n",
    "X = data_encoded.drop('loan_approved', axis=1)\n",
    "y = data_encoded['loan_approved'] # Target variable\n",
    "\n",
    "# Get feature names after one-hot encoding for consistency\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "print(\"\\nSample of the encoded dataset (features only):\")\n",
    "print(X.head())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 3. Split Data into Training and Testing Sets\n",
    "# Using stratify=y is crucial for imbalanced datasets like loan approval/denial\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)} samples\")\n",
    "print(f\"Testing set size: {len(X_test)} samples\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- Hyperparameter Tuning with Cross-Validation (GridSearchCV) ---\n",
    "print(\"Starting Hyperparameter Tuning for Random Forest...\")\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300], # Number of trees in the forest\n",
    "    'max_depth': [5, 10, 15, None], # Maximum depth of the tree\n",
    "    'min_samples_leaf': [1, 5, 10], # Minimum number of samples required to be at a leaf node\n",
    "    'min_samples_split': [2, 5, 10], # Minimum number of samples required to split an internal node\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Use StratifiedKFold for cross-validation, essential for imbalanced classification\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=cv_strategy,\n",
    "    scoring='roc_auc', # ROC AUC is a robust metric for imbalanced classes\n",
    "    n_jobs=-1, # Use all available CPU cores\n",
    "    verbose=1 # Show progress\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nHyperparameter Tuning Complete!\")\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best ROC AUC score (cross-validated): {grid_search.best_score_:.4f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 4. Train the Best Random Forest Model\n",
    "model = grid_search.best_estimator_ # Get the best model found by GridSearchCV\n",
    "print(f\"Using best model with parameters: {model.get_params()}\")\n",
    "# The model is already fitted from GridSearchCV's .fit() call\n",
    "\n",
    "print(\"Best Random Forest Model Trained Successfully!\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 5. Make Predictions on the Test Set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1] # Probability of loan approval (class 1)\n",
    "\n",
    "print(\"Sample Predictions on Test Set:\")\n",
    "results = pd.DataFrame({\n",
    "    'Actual Approval': y_test,\n",
    "    'Predicted Approval': y_pred,\n",
    "    'Approval Probability': y_pred_proba\n",
    "})\n",
    "print(results.head(10)) # Display first 10 predictions\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 6. Evaluate Model Performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred, target_names=['Denied', 'Approved'])\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Model Accuracy on Test Set: {accuracy:.4f}\")\n",
    "print(f\"Model ROC AUC Score on Test Set: {roc_auc:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Visualize Confusion Matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Predicted Denied', 'Predicted Approved'],\n",
    "            yticklabels=['Actual Denied', 'Actual Approved'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Housing Loan Approval Prediction')\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 7. Feature Importance\n",
    "print(\"Feature Importances (how much each feature contributes to prediction):\")\n",
    "feature_importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "print(feature_importances)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.barplot(x=feature_importances.values, y=feature_importances.index, palette='crest')\n",
    "plt.title('Feature Importances for Housing Loan Approval Prediction')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 8. Predict for a New Applicant with Input Validation\n",
    "def predict_loan_approval(credit_score, annual_income_k, loan_amount_k, employment_status,\n",
    "                          property_value_k, debt_to_income_ratio,\n",
    "                          model, feature_columns):\n",
    "    # Input Validation (basic checks for logical ranges)\n",
    "    if not (300 <= credit_score <= 850):\n",
    "        print(f\"Warning: Credit Score {credit_score} is outside the typical range (300-850).\")\n",
    "    if not (annual_income_k >= 0):\n",
    "        print(f\"Warning: Annual Income {annual_income_k} should be non-negative.\")\n",
    "    if not (loan_amount_k >= 0):\n",
    "        print(f\"Warning: Loan Amount {loan_amount_k} should be non-negative.\")\n",
    "    if employment_status not in ['Employed', 'Self-Employed', 'Unemployed', 'Retired']:\n",
    "        print(f\"Warning: Invalid Employment Status '{employment_status}'. Must be one of: Employed, Self-Employed, Unemployed, Retired.\")\n",
    "        return \"Invalid Input\", 0.0\n",
    "    if not (property_value_k >= 0):\n",
    "        print(f\"Warning: Property Value {property_value_k} should be non-negative.\")\n",
    "    if not (0 <= debt_to_income_ratio <= 1):\n",
    "        print(f\"Warning: Debt-to-Income Ratio {debt_to_income_ratio} is outside the typical range (0-1).\")\n",
    "\n",
    "    # Create a DataFrame for the new applicant\n",
    "    new_applicant_df = pd.DataFrame([{\n",
    "        'credit_score': credit_score,\n",
    "        'annual_income_k': annual_income_k,\n",
    "        'loan_amount_k': loan_amount_k,\n",
    "        'employment_status': employment_status,\n",
    "        'property_value_k': property_value_k,\n",
    "        'debt_to_income_ratio': debt_to_income_ratio\n",
    "    }])\n",
    "\n",
    "    # One-hot encode the new applicant's categorical features\n",
    "    new_applicant_encoded = pd.get_dummies(new_applicant_df, columns=['employment_status'], drop_first=False)\n",
    "\n",
    "    # Reindex to ensure columns match the training data's feature_columns order\n",
    "    # Fill missing columns (e.g., other employment_status categories not present in this single sample) with 0\n",
    "    new_applicant_processed = new_applicant_encoded.reindex(columns=feature_columns, fill_value=0)\n",
    "\n",
    "    # Predict\n",
    "    prediction = model.predict(new_applicant_processed)[0]\n",
    "    probability = model.predict_proba(new_applicant_processed)[0][1] # Probability of approval (class 1)\n",
    "\n",
    "    status = \"APPROVED\" if prediction == 1 else \"DENIED\"\n",
    "    return status, probability\n",
    "\n",
    "# Test with some new applicant examples\n",
    "print(\"\\nPredicting for new loan applicants with input validation:\")\n",
    "\n",
    "# Example 1: High chance of approval\n",
    "status1, prob1 = predict_loan_approval(\n",
    "    credit_score=750, annual_income_k=120, loan_amount_k=200, employment_status='Employed',\n",
    "    property_value_k=400, debt_to_income_ratio=0.25,\n",
    "    model=model, feature_columns=X.columns\n",
    ")\n",
    "print(f\"Applicant 1 - Predicted Status: {status1}, Probability of Approval: {prob1:.4f}\")\n",
    "\n",
    "# Example 2: High chance of denial\n",
    "status2, prob2 = predict_loan_approval(\n",
    "    credit_score=550, annual_income_k=40, loan_amount_k=300, employment_status='Unemployed',\n",
    "    property_value_k=350, debt_to_income_ratio=0.55,\n",
    "    model=model, feature_columns=X.columns\n",
    ")\n",
    "print(f\"Applicant 2 - Predicted Status: {status2}, Probability of Approval: {prob2:.4f}\")\n",
    "\n",
    "# Example 3: Borderline case\n",
    "status3, prob3 = predict_loan_approval(\n",
    "    credit_score=680, annual_income_k=70, loan_amount_k=280, employment_status='Self-Employed',\n",
    "    property_value_k=300, debt_to_income_ratio=0.40,\n",
    "    model=model, feature_columns=X.columns\n",
    ")\n",
    "print(f\"Applicant 3 - Predicted Status: {status3}, Probability of Approval: {prob3:.4f}\")\n",
    "\n",
    "# Example 4: With problematic input (will trigger warnings)\n",
    "status4, prob4 = predict_loan_approval(\n",
    "    credit_score=900, annual_income_k=-50, loan_amount_k=1000, employment_status='Student', # Invalid status\n",
    "    property_value_k=0, debt_to_income_ratio=1.5,\n",
    "    model=model, feature_columns=X.columns\n",
    ")\n",
    "print(f\"Applicant 4 - Predicted Status: {status4}, Probability of Approval: {prob4:.4f}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n--- Summary of Housing Loan Approval Prediction with Random Forest ---\")\n",
    "print(\"1. **Data Generation**: Simulated realistic financial and personal data for loan applicants, including categorical employment status.\")\n",
    "print(\"2. **Preprocessing**: Used one-hot encoding for the `employment_status` categorical feature, which is essential for Random Forest.\")\n",
    "print(\"3. **Hyperparameter Tuning**: Applied `GridSearchCV` with `StratifiedKFold` to find the optimal Random Forest parameters, focusing on `roc_auc` for robust evaluation in binary classification.\")\n",
    "print(\"4. **Model Evaluation**: Provided comprehensive evaluation metrics including accuracy, confusion matrix, classification report, and ROC curve, which are critical for assessing model performance in a sensitive application like loan approval.\")\n",
    "print(\"5. **Feature Importance**: Identified the most influential factors (e.g., credit score, DTI, income) in determining loan approval, offering valuable insights for lenders.\")\n",
    "print(\"6. **Prediction Function with Validation**: Developed a `predict_loan_approval` function that includes input validation and correctly handles one-hot encoding for new, unseen data, making it more robust for deployment.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
