{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: Calculate R2 Score <br>\n",
    "\n",
    "Description:<br>\n",
    "Calculate the R2 score for a set of predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Question 3: Calculate R2 Score ---\n",
      "Objective: Calculate the R2 score for a set of predicted values.\n",
      "------------------------------------------------------------------\n",
      "\n",
      "--- Scenario: Perfect Prediction ---\n",
      "Actual Values (y_true): [10 20 30 40 50]\n",
      "Predicted Values (y_pred): [10 20 30 40 50]\n",
      "Calculated R2 Score: 1.0000\n",
      "Interpretation: Excellent fit. The model explains a very high proportion of the variance in the target variable.\n",
      "\n",
      "--- Scenario: Good Prediction ---\n",
      "Actual Values (y_true): [10 20 30 40 50 60 70 80]\n",
      "Predicted Values (y_pred): [11 19 32 38 51 59 68 79]\n",
      "Calculated R2 Score: 0.9960\n",
      "Interpretation: Excellent fit. The model explains a very high proportion of the variance in the target variable.\n",
      "\n",
      "--- Scenario: Poor Prediction (Random) ---\n",
      "Actual Values (y_true): [10 20 30 40 50 60 70 80]\n",
      "Predicted Values (y_pred): [25  5 60 15 70 30 10 45]\n",
      "Calculated R2 Score: -0.9286\n",
      "Interpretation: Very poor fit. The model performs worse than simply predicting the mean of the target variable. This indicates the model is fundamentally flawed for this data.\n",
      "\n",
      "--- Scenario: Predicting the Mean ---\n",
      "Actual Values (y_true): [10 20 30 40 50]\n",
      "Predicted Values (y_pred): [30. 30. 30. 30. 30.]\n",
      "Calculated R2 Score: 0.0000\n",
      "Interpretation: Moderate to poor fit. The model explains some, little, or none of the variance in the target variable compared to simply predicting the mean.\n",
      "\n",
      "--- Scenario: Worse than Mean Prediction ---\n",
      "Actual Values (y_true): [10 20 30 40 50]\n",
      "Predicted Values (y_pred): [  1 100   5  80   0]\n",
      "Calculated R2 Score: -10.2060\n",
      "Interpretation: Very poor fit. The model performs worse than simply predicting the mean of the target variable. This indicates the model is fundamentally flawed for this data.\n",
      "\n",
      "--- Scenario: Constant True Values (Perfect Pred) ---\n",
      "Actual Values (y_true): [50 50 50 50 50]\n",
      "Predicted Values (y_pred): [50 50 50 50 50]\n",
      "Calculated R2 Score: 1.0000\n",
      "Interpretation: Excellent fit. The model explains a very high proportion of the variance in the target variable.\n",
      "\n",
      "--- Scenario: Constant True Values (Imperfect Pred) ---\n",
      "Actual Values (y_true): [50 50 50 50 50]\n",
      "Predicted Values (y_pred): [51 49 50 51 49]\n",
      "Calculated R2 Score: 0.0000\n",
      "Interpretation: Moderate to poor fit. The model explains some, little, or none of the variance in the target variable compared to simply predicting the mean.\n",
      "\n",
      "------------------------------------------------------------------\n",
      "Key Takeaways on R2 Score:\n",
      " - R2 ranges from -infinity to 1. A higher R2 indicates a better fit.\n",
      " - It represents the proportion of variance in the dependent variable predictable from the independent variable(s).\n",
      " - R2 = 1: Perfect fit (all variance explained).\n",
      " - R2 = 0: The model explains no variance (as good as predicting the mean).\n",
      " - R2 < 0: The model is worse than predicting the mean (e.g., due to poor model choice or overfitting/underfitting).\n",
      " - It's important to consider R2 alongside other metrics (MAE, MSE, RMSE) and the context of your data, as a high R2 doesn't always guarantee a good model (e.g., overfitting or non-linear data fitted with linear model).\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(\"--- Question 3: Calculate R2 Score ---\")\n",
    "print(\"Objective: Calculate the R2 score for a set of predicted values.\")\n",
    "print(\"------------------------------------------------------------------\")\n",
    "\n",
    "def calculate_and_print_r2(y_true, y_pred, scenario_name):\n",
    "    \"\"\"\n",
    "    Calculates the R2 score and prints it along with an interpretation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        print(f\"\\n--- Scenario: {scenario_name} ---\")\n",
    "        print(f\"Actual Values (y_true): {y_true}\")\n",
    "        print(f\"Predicted Values (y_pred): {y_pred}\")\n",
    "        print(f\"Calculated R2 Score: {r2:.4f}\")\n",
    "\n",
    "        if r2 >= 0.75:\n",
    "            print(\"Interpretation: Excellent fit. The model explains a very high proportion of the variance in the target variable.\")\n",
    "        elif r2 >= 0.5:\n",
    "            print(\"Interpretation: Good fit. The model explains a significant portion of the variance in the target variable.\")\n",
    "        elif r2 >= 0:\n",
    "            print(\"Interpretation: Moderate to poor fit. The model explains some, little, or none of the variance in the target variable compared to simply predicting the mean.\")\n",
    "        else:\n",
    "            print(\"Interpretation: Very poor fit. The model performs worse than simply predicting the mean of the target variable. This indicates the model is fundamentally flawed for this data.\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"\\nERROR: Could not calculate R2 for scenario '{scenario_name}': {e}. Check if y_true and y_pred have compatible shapes or contain invalid values.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: An unexpected error occurred for scenario '{scenario_name}': {e}\")\n",
    "\n",
    "# --- Scenario 1: Perfect Prediction (R2 = 1.0) ---\n",
    "y_true_perfect = np.array([10, 20, 30, 40, 50])\n",
    "y_pred_perfect = np.array([10, 20, 30, 40, 50])\n",
    "calculate_and_print_r2(y_true_perfect, y_pred_perfect, \"Perfect Prediction\")\n",
    "\n",
    "# --- Scenario 2: Good Prediction (R2 > 0.5) ---\n",
    "y_true_good = np.array([10, 20, 30, 40, 50, 60, 70, 80])\n",
    "y_pred_good = np.array([11, 19, 32, 38, 51, 59, 68, 79]) # Close predictions\n",
    "calculate_and_print_r2(y_true_good, y_pred_good, \"Good Prediction\")\n",
    "\n",
    "# --- Scenario 3: Poor Prediction (R2 close to 0) ---\n",
    "y_true_poor = np.array([10, 20, 30, 40, 50, 60, 70, 80])\n",
    "y_pred_poor = np.array([25, 5, 60, 15, 70, 30, 10, 45]) # Wildly off, somewhat random\n",
    "calculate_and_print_r2(y_true_poor, y_pred_poor, \"Poor Prediction (Random)\")\n",
    "\n",
    "# --- Scenario 4: Predicting the Mean (R2 = 0) ---\n",
    "# If your model simply predicts the mean of the true values, R2 will be 0.\n",
    "y_true_mean = np.array([10, 20, 30, 40, 50])\n",
    "y_pred_mean = np.array([np.mean(y_true_mean), np.mean(y_true_mean), np.mean(y_true_mean), np.mean(y_true_mean), np.mean(y_true_mean)])\n",
    "calculate_and_print_r2(y_true_mean, y_pred_mean, \"Predicting the Mean\")\n",
    "\n",
    "# --- Scenario 5: Worse than Predicting the Mean (Negative R2) ---\n",
    "# This happens when your predictions are consistently worse than just using the average.\n",
    "y_true_worse = np.array([10, 20, 30, 40, 50])\n",
    "y_pred_worse = np.array([1, 100, 5, 80, 0]) # Very far from true values, worse than mean\n",
    "calculate_and_print_r2(y_true_worse, y_pred_worse, \"Worse than Mean Prediction\")\n",
    "\n",
    "# --- Scenario 6: Edge Case - Constant True Values (R2 can be undefined or 0) ---\n",
    "# If y_true has no variance, R2 is undefined or 0 because the denominator is zero.\n",
    "# Scikit-learn handles this by returning 0.0 or a specific error if `multioutput` is not 'raw_values'.\n",
    "y_true_constant = np.array([50, 50, 50, 50, 50])\n",
    "y_pred_constant_perfect = np.array([50, 50, 50, 50, 50])\n",
    "y_pred_constant_imperfect = np.array([51, 49, 50, 51, 49]) # Even slightly off predictions\n",
    "calculate_and_print_r2(y_true_constant, y_pred_constant_perfect, \"Constant True Values (Perfect Pred)\")\n",
    "calculate_and_print_r2(y_true_constant, y_pred_constant_imperfect, \"Constant True Values (Imperfect Pred)\")\n",
    "\n",
    "\n",
    "print(\"\\n------------------------------------------------------------------\")\n",
    "print(\"Key Takeaways on R2 Score:\")\n",
    "print(\" - R2 ranges from -infinity to 1. A higher R2 indicates a better fit.\")\n",
    "print(\" - It represents the proportion of variance in the dependent variable predictable from the independent variable(s).\")\n",
    "print(\" - R2 = 1: Perfect fit (all variance explained).\")\n",
    "print(\" - R2 = 0: The model explains no variance (as good as predicting the mean).\")\n",
    "print(\" - R2 < 0: The model is worse than predicting the mean (e.g., due to poor model choice or overfitting/underfitting).\")\n",
    "print(\" - It's important to consider R2 alongside other metrics (MAE, MSE, RMSE) and the context of your data, as a high R2 doesn't always guarantee a good model (e.g., overfitting or non-linear data fitted with linear model).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
